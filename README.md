## EURON 3ê¸° ì‹¬í™”í•™ìŠµ íŒ€ - 11ì£¼ì°¨

* 12 ì›” 2ì¼ 

<details>
<summary>ML</summary>
<div markdown="1">       

<br />  
  
| ì£¼ì°¨ | ë‚´ìš©         | ë°œí‘œì                       | ë°œí‘œìë£Œ |
| ---- | ------------ | ---------------------------- | -------- |
| 11    | ë”¥ëŸ¬ë‹ íŒŒì´í† ì¹˜ êµê³¼ì„œ 8ì¥  | ì˜¤ìˆ˜ì§„, ê¹€ì˜ˆì§„, ë°•ë³´ì˜ | [ğŸ“š]()    |

  
## Assignment

### ğŸ“ ì˜ˆìŠµê³¼ì œ

  * ë”¥ëŸ¬ë‹ íŒŒì´í† ì¹˜ êµê³¼ì„œ 8ì¥ ì •ë¦¬

### ğŸ“ ë³µìŠµê³¼ì œ

   * [Bidirectional LSTM Networkë¥¼ ì´ìš©í•œ PoS Tagging](https://ws-choi.github.io/blog-kor/nlp/deeplearning/Pos-Tagging-with-Bidirectional-LSTM/) ì½”ë“œ í•„ì‚¬/ë¶„ì„
   * [Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/pdf/2205.13504v3.pdf) ë…¼ë¬¸ ì½ê³  ì •ë¦¬
      - `Attention`ê³¼ `Transformer`ì€ í›„ë°˜ë¶€ ìì—°ì–´ì²˜ë¦¬ ì±•í„°ì—ì„œ ê°„ë‹¨íˆ ë‹¤ë£¨ê³  ë„˜ì–´ê°€ê¸° ë•Œë¬¸ì—, ì´ë²ˆ ë³µìŠµê³¼ì œì—ì„œ ê°œë…ì„ ì¡ê³  ìœ„ ë…¼ë¬¸ì„ ì½ì–´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.
         - [Attention ì°¸ê³  ì‚¬ì´íŠ¸](https://nlpinkorean.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)  
         - [Transformer ì°¸ê³  ì‚¬ì´íŠ¸](https://nlpinkorean.github.io/illustrated-transformer/)
      


  
</div>
</details>



<details>
<summary>DL</summary>
<div markdown="1">       

<br />  
  
| ì£¼ì°¨ | ë‚´ìš©         | ë°œí‘œì                       | ë°œí‘œìë£Œ |
| ---- | ------------ | ---------------------------- | -------- |
| 11   |  |   | [ğŸ“š]()    |

  
* 9ì£¼ì°¨ ë‚´ìš© ë³µìŠµê³¼ì œ

[GNN1](https://github.com/mnslarcher/cs224w-slides-to-code/blob/main/notebooks/06-graph-neural-networks-1-gnn-model.ipynb) ê°œë… ë³µìŠµ ë° ì½”ë“œ í•„ì‚¬/ë¶„ì„í•˜ê¸° 

  
</div>
</details>
